%!TEX root = ../dokumentation.tex

\chapter{Fazit} \label{ch:conclusion}

Abschließend werden zunächst die Erkenntnisse und Abläufe dieser Arbeit zusammengefasst. In der anschließenden Reflexion werden zum einen die Forschungsfragen beantwortet und die Ergebnisse bewertet. Schlussendlich werden im Ausblick offene Untersuchungen dokumentiert, die sich in dieser Arbeit ergeben haben oder außerhalb des Umfanges dieser Arbeit lagen.

\section{Zusammenfassung}

Wie in \autoref{subsec:heterogenitätParteien} dargelegt, koalieren die Union und \ac{SPD} in der \num{19}. Wahlperiode. Die Opposition besteht demnach aus den Grünen, \ac{FDP}, Linken und der \ac{AfD}. In dem Untersuchungszeitraum dominieren Themen wie Migration, Klimaschutz und Pandemie den politischen Diskurs. In \autoref{ch:crispDm_1} werden zunächst die Tweets, Reden und Wahlprogramme analysiert und bereinigt. Durch die Bereinigung hat sich die Anzahl an Tweet drastisch verringert. Die Wahlprogramm-Paragraphen sind gleich geblieben, während die Reden durch die Aufteilung mehr sind als zuvor. In der Analyse zeigen sich textsortenspezifische Muster bei der Länge und dem Sentiment der Dokumente. Mittels simpler Wortwolken lassen sich erste Merkmale zur Klassifikation der einzelnen Parteien ableiten. Anhand des Sentiments kann auf allen Textsorten eine Trennung der Parteien nach Regierung und Opposition vorgenommen werden.

In \autoref{ch:crispDm_2} zeigt sich, dass die Modelle Schwierigkeiten dabei haben, Parteien, die sich politisch nahe stehen, eindeutig zu klassifizieren. Bei dem Trainieren der Modelle treten vermehrt Limitationen bezüglich der verfügbaren Ressourcen auf. Insgesamt schneiden die Transformer-Modelle am besten ab, wobei sich lediglich das DistilBERT Modell auf dem gesamten Datensatz trainieren lässt. Auf dem unbalancierten und balancierten Datensatz erreicht das Modell \(F_1\) Scores zwischen \numrange{0.58}{0.60}. Besonders die \ac{AfD} lässt sich am akkuratesten klassifizieren, wohingegen andere, politisch nahestehende Parteien zu Verwechselungen neigen.

Anschließend werden weitere Untersuchungen zu Thesen vorgenommen, die zuvor definiert wurden, oder sich im Laufe der Arbeit ergeben haben. Alle drei Untersuchungen fallen positiv aus.

\section{Reflexion}

Die vorliegende Arbeit weist nach, dass unterschiedliche Texte von Politikern dazu genutzt werden können, um ein Klassifikationsmodell für die sechs Fraktionen des Deutschen Bundestags der \num{19}. Wahlperiode zu trainieren. Die verschiedenen Modelle erreichen auf dem kombinierten, balancierten Datensatz \(F_1\) Scores von bis zu \num{0.58}. Neben einer guten Performance war eine weitere Anforderung, dass das Modell eine kurze Trainings- und Inferenz-Zeit aufweist. \ft erfüllt beide dieser Kriterien. Schlussendlich erschien es jedoch wichtiger ein Modell zu wählen, dass komplexere Satzstrukturen besser erfassen kann. DistilBERT stellt somit einen Kompromiss dar. Es trainiert bei Weitem langsamer als \ft und doch schneller als größere Sprachmodelle.

Dennoch stellen die Daten in Kombination mit den verwendeten Modellen eine Herausforderung dar. Aus \autoref{subsec:furtherExperiments} geht hervor, dass das Modell unbekannte Textsorten signifikant schlechter klassifizieren kann. Daraus folgt, dass das bereitgestellte Modell in der aktuellen Version nicht für unbekannte Arten von Text geeignet ist. Zusätzlich dazu zeigt sich, die Modellperformance in Form des \(F_1\) Scores mit steigender Größe des Untersuchungszeitraums abnimmt. Durch die Einschränkung auf die \num{19}. Wahlperiode sind die resultierenden Modelle nur auf die Themen aus diesem Zeitraum ausgerichtet.

Zusammenfassend ist festzustellen, dass die Modelle anfällig für zeitliche Abweichungen, alternative Textsorten, wechselnde politische Themen und die Länge von Texten sind. Ferner ist anzunehmen, dass abweichende Regierungskonstellationen ebenfalls einen signifikanten Effekt auf den politischen Diskurs und somit die Modellperformance haben.

\section{Ausblick}

Ausgehend von den in dieser Arbeiten durchgeführten Analysen und Untersuchungen ergeben sich weitere Ansatzpunkte, von denen ausgehend verwandte Forschungsfragen untersucht werden können.

Die Untersuchung in \autoref{subsec:outOfDomain} zeigt, dass die Größe des Trainings-Datensatzes einen Einfluss auf die resultierende Modellperformance. Um die Datenmenge zu erhöhen, könnten in weiteren Untersuchung ebenfalls Retweets für das Training genutzt werden. Duplikate sollten dennoch herausgefiltert werden. Auch bei den anderen Datensätzen könnte die Filterung gelockert werden, um weniger Einträge verwerfen zu müssen. Bei den Wahlprogrammen sowie bei den Reden käme eine Erhöhung der Mindestlänge der Texte infrage. Es besteht dabei das Risiko, dass irrelevante Texte hinzukommen und einen Störfaktor beim Training darstellen. Weitere Möglichkeiten, um die Informationsdichte der vorhandenen, bereinigten Daten zu erhöhen, wäre es, Emojis in Klartext umzuwandeln. Emojis sind, besonders auf Plattformen wie Twitter, ein sprachliches Mittel und enthalten weitere Informationen über den Kontext und Sentiment eines Textes \autocite{guhr_training_2020}.

Es bietet sich an, weitere Textsorten in die Auswahl an Datensätzen für Training aufzunehmen. Die Untersuchung zu der Performance des \ft Modells auf out-of-domain Daten hat gezeigt, dass Klassifikationen ohne bedeutenden Verlust an Genauigkeit nur auf Textsorten möglich ist, auf denen auch trainiert wurde. Weitere Arten von Text würden die generalisierte Nutzbarkeit des auf dem kombinierten Datensatz trainierten Modells steigern.

Wie in \autoref{subsec:discussion} angedeutet, kann eine neutrale Klasse hinzugefügt werden. Es sollte sichergestellt werden, dass die Trainingsdaten nicht nur von einem Politiker verfasst wurden, sondern einen eindeutigen politischen Kontext aufweisen. Hierfür könnten Modelle verwendet werden, die feststellen, ob ein Text Fakten enthält oder nicht. Für englische Texte gibt es solche Modelle bereits. Ein alternativer Ansatz zum Hinzufügen einer neutralen Klasse wäre es auch Datensätze zu inkludieren, die noch keinem Politiker oder Partei zugeordnet sind. Diese müssen voraussichtlich aber manuell gekennzeichnet werden.

Anhand der durchgeführten Trainings wird ersichtlich, dass die Klassifikation von mehreren Klassen zu einer zusätzlichen Komplexität führt. Alternativ dazu könnten die Modelle nicht auf die Parteien, sondern Parteigruppen -- beispielsweise links und rechts -- trainiert werden. Ferner sind auch von Parteien unabhängige Klassifikationen denkbar, wie zwischen politischen und unpolitisch Texten oder solchen, die einen Fakt enthalten oder nicht. Diese wären ebenfalls dazu geeignet, um Einschätzungen über neue, noch nicht betrachtete Texte oder Medien zu bekommen.

Auch können weitere Metainformationen für das Training der Modelle verwendet werden. Mithilfe des Multimodal Transformers Projekts\footnote{\href{https://github.com/jianzhnie/MultimodalTransformers}{https://github.com/jianzhnie/MultimodalTransformers}} könnten zusammen mit dem reinen Text andere numerische oder kategoriale Features, wie das Sentiment des Textes, die Faktendichte oder die Tatsache, ob im Text gegendert wird, helfen, die Modellperformance zu verbessern.

An mehreren Stellen dieser Arbeit werden einschränkende Maßnahmen getroffen, weil die zur Verfügung stehenden Ressourcen für die gewollten Konfigurationen beim Training nicht ausreichen. Bevor jedoch die Modelle auf mehr Daten trainiert werden können, ist es relevant, die Ressourcen für das Training entweder vertikal oder horizontal zu skalieren. Zusätzlich könnten mit mehr Ressourcen ebenfalls neuere State-of-the-Art Verfahren genutzt werden.
