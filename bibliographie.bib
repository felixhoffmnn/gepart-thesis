
@online{noauthor_112_nodate,
	title = {1.12. Multiclass and multioutput algorithms},
	url = {https://scikit-learn/stable/modules/multiclass.html},
	abstract = {This section of the user guide covers functionality related to multi-learning problems, including multiclass, multilabel, and multioutput classification and regression. The modules in this section ...},
	titleaddon = {scikit-learn},
	urldate = {2023-05-31},
	langid = {english},
}

@online{google_prepare_2022,
	title = {Prepare Your Data},
	url = {https://developers.google.com/machine-learning/guides/text-classification/step-3},
	titleaddon = {Google Text Classification Guide},
	author = {{Google}},
	urldate = {2023-05-31},
	date = {2022-07-18},
	langid = {english},
}

@misc{yang_harnessing_2023,
	title = {Harnessing the Power of {LLMs} in Practice: A Survey on {ChatGPT} and Beyond},
	url = {http://arxiv.org/abs/2304.13712},
	shorttitle = {Harnessing the Power of {LLMs} in Practice},
	abstract = {This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models ({LLMs}) in their downstream natural language processing ({NLP}) tasks. We provide discussions and insights into the usage of {LLMs} from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current {GPT}- and {BERT}-style {LLMs}. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of {LLMs} in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each {NLP} task. Furthermore, we explore the impact of spurious biases on {LLMs} and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying {LLMs} in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with {LLMs}, thereby enabling the successful implementation of these models in a wide range of {NLP} tasks. A curated list of practical guide resources of {LLMs}, regularly updated, can be found at {\textbackslash}url\{https://github.com/Mooler0410/{LLMsPracticalGuide}\}.},
	number = {{arXiv}:2304.13712},
	publisher = {{arXiv}},
	author = {Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Yin, Bing and Hu, Xia},
	urldate = {2023-05-29},
	date = {2023-04-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2304.13712 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{simoes_fine-tuned_2020,
	title = {Fine-Tuned {BERT} for the Detection of Political Ideology},
	url = {https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/reports/custom/report43.pdf},
	abstract = {In this paper, we apply Bidirectional Encoder Representations from Transformers ({BERT}) to detect political ideologies in congressional debate transcripts from 2005. For this task, the Ideological Books Corpus ({IBC}) data set was used, which contains 4,062 sentences annotated for political ideology. Using ﬁne tuned {BERT} the accuracy achieved was 68\%. The F1 Score achieved was 65\%, which is more than 30 percentage points higher than former implementations.},
	author = {Simoes, Alexandre and Castaños, Maria del Mar},
	date = {2020},
	langid = {english},
}

@online{noauthor_bundestagswahl_nodate,
	title = {Bundestagswahl 2017},
	url = {https://www.bundeswahlleiter.de/bundestagswahlen/2017/ergebnisse/bund-99.html},
	titleaddon = {Die Bundeswahlleiterin},
	urldate = {2023-04-12},
}

@online{forschungsgruppe_wahlen_forschungsgruppe_nodate,
	title = {Forschungsgruppe Wahlen: Politbarometer},
	url = {https://www.forschungsgruppe.de/Umfragen/Politbarometer/Langzeitentwicklung_-_Themen_im_Ueberblick/Politik_II/},
	author = {{Forschungsgruppe Wahlen}},
	urldate = {2023-04-12},
}

@online{noauthor_bundestagswahl_2017,
	title = {Bundestagswahl 2017: Endgültiges Ergebnis - Die Bundeswahlleiterin},
	url = {https://www.bundeswahlleiter.de/info/presse/mitteilungen/bundestagswahl-2017/34_17_endgueltiges_ergebnis.html},
	urldate = {2023-04-07},
	date = {2017-10-17},
}

@incollection{niedermayer_entwicklung_2020,
	location = {Wiesbaden},
	title = {Die Entwicklung des Parteiensystems bis nach der Bundestagswahl 2017},
	isbn = {978-3-658-29771-8},
	url = {https://doi.org/10.1007/978-3-658-29771-8_1},
	abstract = {Das deutsche Parteiensystem hat mit der Bundestagswahl 2017 wohl endgültig einen Typwechsel von einem System mit Zweiparteiendominanz zu einem pluralistischen System vollzogen und in den zwei Jahren nach der Wahl zeigten sich weitere gravierende Veränderungen. Das Kapitel erläutert zunächst die einzelnen strukturellen und inhaltlichen Parteiensystemeigenschaften sowie die Systemtypologie und zeigt deren Veränderungen über die Zeit hinweg auf. Danach wird analysiert, warum es zu dem Typwechsel kam. Die wesentlichen Faktoren sind dabei einerseits der längerfristige Wandel der Rahmenbedingungen und der Nachfrageseite des politischen Wettbewerbs durch Prozesse des ökonomischen und sozialen Wandels und andererseits kurzfristige Faktoren der Angebotsseite in Gestalt von Schlüsselentscheidungen der Parteien, die ihr personelles und inhaltliches Angebot an die Wähler betreffen.},
	pages = {1--41},
	booktitle = {Die Parteien nach der Bundestagswahl 2017: Aktuelle Entwicklungen des Parteienwettbewerbs in Deutschland},
	publisher = {Springer Fachmedien},
	author = {Niedermayer, Oskar},
	editor = {Jun, Uwe and Niedermayer, Oskar},
	urldate = {2023-04-07},
	date = {2020},
	langid = {german},
	doi = {10.1007/978-3-658-29771-8_1},
}

@online{dr_christian_marettek_bundestag_nodate,
	title = {Bundestag: Wahlperiode 2017-2021},
	url = {https://www.demokratie-leben.org/wahlperiode-2017-2021/},
	shorttitle = {Bundestag},
	abstract = {Forschungsinstitut für Demokratieforschung. Die Probleme der Gesellschaft herausfinden ist unser Ziel.},
	type = {Forschungsinstitut Demokratie leben, Saarbrücken ({FIDES} e.V.)},
	author = {{Dr. Christian Marettek}},
	urldate = {2023-04-07},
	langid = {german},
}

@incollection{engler_wettbewerb_2022,
	location = {Wiesbaden},
	title = {Wettbewerb um Wählerstimmen, Klimakrise und die Corona-Pandemie. Parteienwettbewerb und Regierungshandeln in der 19. Wahlperiode},
	isbn = {978-3-658-38002-1},
	url = {https://doi.org/10.1007/978-3-658-38002-1_4},
	abstract = {Dieser Beitrag befasst sich mit den Auswirkungen des Parteienwettbewerbs auf das Regierungshandeln in den beiden salientesten Politikbereichen der 19. Legislaturperiode: Klimaschutz und Bekämpfung der Corona-Pandemie. Unsere Untersuchung der Nachfrageseite des politischen Marktes stellt fest, dass der Ausgang der Bundestagswahl 2021 für die Parteien der Großen Koalition lange Zeit offen und eine erneute Regierungsbeteiligung nicht sicher war. Auf der Angebotsseite zeigt sich, dass in der Umweltpolitik Bündnis 90/Die Grünen eine glaubwürdige und für kompetent erachtete Policy-Alternative boten, während in der Politik der Pandemiebekämpfung keine der parlamentarischen Oppositionsparteien umfangreiche Kompetenzwerte für sich reklamieren konnte. Für die daraus ableitbare Erwartung, wonach die Umweltpolitik, weniger jedoch die Corona-Krisenpolitik, vom Parteienwettbewerb hätte bestimmt sein sollen, finden wir in unseren quantitativen und qualitativen Analysen allerdings nur eingeschränkt Belege: Auf der einen Seite war die Corona-Politik lange Zeit dem Parteienwettbewerb weitgehend entzogen. Auf der anderen Seite gibt es auch in der Umwelt- und Klimaschutzpolitik nur einzelne Hinweise auf Effekte des Wettbewerbs um Wählerstimmen. Die Auswirkungen des Parteienwettbewerbs auf Kommunikation und Politikentscheidungen der vierten Regierung Merkel in den betrachteten Politikfeldern scheinen daher insgesamt eher begrenzt gewesen zu sein.},
	pages = {69--99},
	booktitle = {Das Ende der Merkel-Jahre: Eine Bilanz der Regierung Merkel 2018-2021},
	publisher = {Springer Fachmedien},
	author = {Engler, Fabian and Zohlnhöfer, Reimut},
	editor = {Zohlnhöfer, Reimut and Engler, Fabian},
	urldate = {2023-04-07},
	date = {2022},
	langid = {german},
	doi = {10.1007/978-3-658-38002-1_4},
	keywords = {Corona-Pandemie, Klimaschutz, Merkel, Parteienwettbewerb},
}

@inproceedings{banisch_wer_2023,
	title = {Wer redet über was im Bundestag? Die 19. Legislaturperiode durch die Brille semantischer Netzwerke},
	url = {https://stufo2022.berlinexchange.de/pub/qlfifkqf/release/1},
	shorttitle = {Wer redet über was im Bundestag?},
	abstract = {Mithilfe von semantischen Netzwerken und Natural Language Processing haben wir uns den 24.666 während der 19. Legislaturperiode im Bundestag gehaltenen Reden genähert.},
	eventtitle = {{StuFo} 2022 Conference Proceedings},
	booktitle = {Proceedings - {StuFo} Conference 2022},
	publisher = {Berlin Exchange},
	author = {Banisch, Sven and Knoll, Christian and Wessels, Joris},
	urldate = {2023-04-07},
	date = {2023-01-20},
	langid = {english},
}

@incollection{thomeczek_politische_2019,
	location = {Wiesbaden},
	title = {Die politische Landschaft zur Bundestagswahl 2017},
	isbn = {978-3-658-25050-8},
	url = {https://doi.org/10.1007/978-3-658-25050-8_12},
	abstract = {Ziel des Beitrages ist eine Analyse der politischen Landschaft zur Bundestagswahl 2017. Dazu werden sowohl Daten aus gängigen Expertenbefragungen als auch aus Wahlhilfen („Voting Advice Applications“; {VAAs}) herangezogen. Der Beitrag zeigt, dass die politische Landschaft 2017 in zwei Lager, das „bürgerliche“ und das „linke“ Lager, gespalten war, was im Wesentlichen auch der Ausgangslage der Bundestagswahl 2013 entspricht. Weiterhin können wir zeigen, dass die Nutzer der {VAA} „Bundeswahlkompass“ sich tendenziell näher an der politischen Mitte verorten als die von ihnen präferierten Parteien. Abschließend identifizieren wir anhand einer Analyse der Kandidatendaten zur Bundestagswahl mithilfe der Daten der {VAA} „Kandidaten-Check“ Themen, die auf einen hohen innerparteilichen Konsens bzw. Dissens hinweisen.},
	pages = {267--291},
	booktitle = {Die Bundestagswahl 2017: Analysen der Wahl-, Parteien-, Kommunikations- und Regierungsforschung},
	publisher = {Springer Fachmedien},
	author = {Thomeczek, Jan Philipp and Jankowski, Michael and Krouwel, André},
	editor = {Korte, Karl-Rudolf and Schoofs, Jan},
	urldate = {2023-04-07},
	date = {2019},
	langid = {german},
	doi = {10.1007/978-3-658-25050-8_12},
}

@online{schmid_deutscher_2021,
	title = {Deutscher Bundestag - Im Kampfmodus gegen die Corona-Pandemie (2017 bis 2021)},
	url = {https://www.bundestag.de/dokumente/textarchiv/19-wahlperiode-865198},
	abstract = {Die Corona-Krise und der erstmalige Einzug der {AfD} in den Bundestag waren zwei Ereignisse, welche die 19. Wahlperiode des Deutschen Bundestages (2017-2021) stark gekennzeichnet haben....},
	titleaddon = {Deutscher Bundestag},
	author = {Schmid, Sandra},
	urldate = {2023-04-07},
	date = {2021-10-07},
	langid = {german},
}

@online{willeke_soziale_2019,
	title = {Soziale Medien – Gefahr oder Chance für die Politik?},
	url = {https://www.uni-hamburg.de/newsroom/forschung/2019/0523-socialmedia-europawahl.html},
	abstract = {Die Kommunikationswissenschaftlerin Prof. Dr. Katharina Kleinen-von Königslöw von der Universität Hamburg über die Rolle Facebook, Instagram \& Co. bei der Europawahl 2019.},
	author = {Willeke, Felix},
	urldate = {2023-03-20},
	date = {2019-05-23},
	langid = {german},
}

@inproceedings{chapman_crisp-dm_2000,
	title = {{CRISP}-{DM} 1.0: Step-by-step data mining guide},
	url = {https://www.kde.cs.uni-kassel.de/wp-content/uploads/lehre/ws2012-13/kdd/files/CRISPWP-0800.pdf},
	shorttitle = {{CRISP}-{DM} 1.0},
	abstract = {This document describes the {CRISP}-{DM} process model, including an introduction to the {CRISP}-{DM} methodology, the {CRISP}-{DM} reference model, the {CRISP}-{DM} user guide and the {CRISP}-{DM} reports, as well as an appendix with additional useful and related information. This document and information herein, are the exclusive property of the partners of the {CRISP}-{DM} All trademarks and service marks mentioned in this document are marks of their respective owners and are as such acknowledged by the members of the {CRISP}-{DM} consortium. Foreword {CRISP}-{DM} was conceived in late 1996 by three " veterans " of the young and immature data mining market. {DaimlerChrysler} (then Daimler-Benz) was already experienced, ahead of most industrial and commercial organizations, in applying data mining in its business operations. {SPSS} (then {ISL}) had been providing services based on data mining since 1990 and had launched the first commercial data mining workbench – Clementine – in 1994. {NCR}, as part of its aim to deliver added value to its Teradata data warehouse customers, had established teams of data mining consultants and technology specialists to service its clients' requirements. At that time, early market interest in data mining was showing signs of exploding into widespread uptake. This was both exciting and terrifying. All of us had developed our approaches to data mining as we went along. Were we doing it right? Was every new adopter of data mining going to have to learn, as we had initially, by trial and error? And from a supplier's perspective, how could we demonstrate to prospective customers that data mining was sufficiently mature to be adopted as a key part of their business processes? A standard process model, we reasoned, non-proprietary and freely available, would address these issues for us and for all practitioners. A year later we had formed a consortium, invented an acronym ({CRoss}-Industry Standard Process for Data Mining), obtained funding from the European Commission and begun to set out our initial ideas. As {CRISP}-{DM} was intended to be industry-, tool-and application-neutral, we knew we had to get input from as wide a range as possible of practitioners and others (such as data warehouse vendors and management consultancies) with a vested interest in data mining. We did this by creating the {CRISP}-{DM} Special Interest Group (" The {SIG} " , as it became known). We launched the {SIG} by broadcasting an invitation to interested parties to join us in Amsterdam for a day-long …},
	author = {Chapman, Pete and Clinton, Julian and Kerber, Randy and Khabaza, Thomas and Reinartz, Thomas and Shearer, Colin and Wirth, Rüdiger},
	urldate = {2023-02-21},
	date = {2000},
}

@inproceedings{biessmann_predicting_2016,
	title = {Predicting political party afﬁliation from text},
	url = {https://ssc.io/publication/predicting-political-party-affiliation-from-text-poltext/},
	abstract = {Every day a large amount of text is produced during public discourse. Some of this text is produced by actors whose political colour is very obvious. However, though many actors cannot clearly be associated with a political party, their statements may be biased towards a speciﬁc party. Identifying such biases is crucial for political research as well as for media consumers, especially when analysing the inﬂuence of the media on political discourse and vice versa. In this study, we investigate the extent to which political party afﬁliation can be predicted from textual content. Results indicate that automated classiﬁcation of political afﬁliation is possible with an accuracy better than chance, even across different text domains. We propose methods to better interpret these results, and ﬁnd that features not related to political policies, such as speech sentiment, can be discriminative and thus exploited by text analysis models.},
	eventtitle = {International Conference on the Advances in Computational Analysis of Political Text},
	author = {Biessmann, Felix and Lehmann, Pola and Kirsch, Daniel and Schelter, Sebastian},
	urldate = {2023-03-11},
	date = {2016},
	langid = {english},
}

@book{jurafsky_speech_2023,
	location = {Upper Saddle River, N.J},
	title = {Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition},
	isbn = {978-0-13-095069-7},
	url = {https://web.stanford.edu/~jurafsky/slp3/ed3book_jan72023.pdf},
	series = {Prentice Hall series in artificial intelligence},
	shorttitle = {Speech and language processing},
	pagetotal = {934},
	publisher = {Prentice Hall},
	author = {Jurafsky, Dan and Martin, James H.},
	date = {2023},
	keywords = {Automatic speech recognition, Computational linguistics},
}

@misc{mikolov_efficient_2013,
	title = {Efficient Estimation of Word Representations in Vector Space},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	number = {{arXiv}:1301.3781},
	publisher = {{arXiv}},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	urldate = {2023-03-08},
	date = {2013-09-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{gu_package_2021,
	location = {Mexico City, Mexico},
	title = {A Package for Learning on Tabular and Text Data with Transformers},
	url = {https://aclanthology.org/2021.maiworkshop-1.10},
	doi = {10.18653/v1/2021.maiworkshop-1.10},
	abstract = {Recent progress in natural language processing has led to Transformer architectures becoming the predominant model used for natural language tasks. However, in many real- world datasets, additional modalities are included which the Transformer does not directly leverage. We present Multimodal- Toolkit, an open-source Python package to incorporate text and tabular (categorical and numerical) data with Transformers for downstream applications. Our toolkit integrates well with Hugging Face's existing {API} such as tokenization and the model hub which allows easy download of different pre-trained models.},
	eventtitle = {maiworkshop 2021},
	pages = {69--73},
	booktitle = {Proceedings of the Third Workshop on Multimodal Artificial Intelligence},
	publisher = {Association for Computational Linguistics},
	author = {Gu, Ken and Budhkar, Akshay},
	urldate = {2023-03-08},
	date = {2021-06},
}

@misc{richter_open_2021,
	title = {Open Discourse},
	url = {https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/FIKIBO},
	doi = {10.7910/DVN/FIKIBO},
	abstract = {Data files of the Open Discourse corpus in different formats},
	publisher = {Harvard Dataverse},
	author = {Richter, Florian and Koch, Philipp and Franke, Oliver and Kraus, Jakob and Kuruc, Fabrizio and Thiem, Anja and Högerl, Judith and Heine, Stella and Schöps, Konstantin},
	urldate = {2023-03-05},
	date = {2021-05-25},
	langid = {english},
	keywords = {Computer and Information Science, Social Sciences},
}

@online{noauthor_word_nodate,
	title = {Word embeddings in {NLP}: A Complete Guide},
	url = {https://www.turing.com/kb/guide-on-word-embeddings-in-nlp},
	shorttitle = {Word embeddings in {NLP}},
	abstract = {Word Embeddings is an advancement in {NLP} that has skyrocketed the ability of computers to understand text-based content. Let's read this article to know more.},
	urldate = {2023-03-01},
	langid = {english},
}

@inproceedings{pennington_glove_2014,
	location = {Doha, Qatar},
	title = {Glove: Global Vectors for Word Representation},
	url = {http://aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	shorttitle = {Glove},
	abstract = {Recent methods for learning vector space representations of words have succeeded in capturing ﬁne-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efﬁciently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
	eventtitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	pages = {1532--1543},
	booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	urldate = {2023-02-28},
	date = {2014},
	langid = {english},
}

@article{moreo_word-class_2021,
	title = {Word-class embeddings for multiclass text classification},
	volume = {35},
	issn = {1384-5810, 1573-756X},
	url = {https://link.springer.com/10.1007/s10618-020-00735-3},
	doi = {10.1007/s10618-020-00735-3},
	pages = {911--963},
	number = {3},
	journaltitle = {Data Mining and Knowledge Discovery},
	shortjournal = {Data Min Knowl Disc},
	author = {Moreo, Alejandro and Esuli, Andrea and Sebastiani, Fabrizio},
	urldate = {2023-02-28},
	date = {2021-05},
	langid = {english},
}

@misc{asenmacher_re-evaluating_2021,
	title = {Re-Evaluating {GermEval}17 Using German Pre-Trained Language Models},
	url = {http://arxiv.org/abs/2102.12330},
	abstract = {The lack of a commonly used benchmark data set (collection) such as (Super) {GLUE} (Wang et al., 2018, 2019) for the evaluation of non-English pre-trained language models is a severe shortcoming of current English-centric {NLP}-research. It concentrates a large part of the research on English, neglecting the uncertainty when transferring conclusions found for the English language to other languages. We evaluate the performance of German and multilingual {BERT} models currently available via the huggingface transformers library on four subtasks of Aspect-based Sentiment Analysis ({ABSA}) from the {GermEval}17 workshop. We compare them to pre-{BERT} architectures (Wojatzki et al., 2017; Schmitt et al., 2018; Attia et al., 2018) as well as to an {ELMobased} architecture (Biesialska et al., 2020) and a {BERT}-based approach (Guhr et al., 2020). The observed improvements are put in relation to those for a similar {ABSA} task (Pontiki et al., 2014) and similar models ({preBERT} vs. {BERT}-based) for the English language and we check whether the reported improvements correspond to those we observe for German.},
	number = {{arXiv}:2102.12330},
	publisher = {{arXiv}},
	author = {Aßenmacher, M. and Corvonato, A. and Heumann, C.},
	urldate = {2023-02-28},
	date = {2021-07-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2102.12330 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{demszky_goemotions_2020,
	title = {{GoEmotions}: A Dataset of Fine-Grained Emotions},
	url = {http://arxiv.org/abs/2005.00547},
	shorttitle = {{GoEmotions}},
	abstract = {Understanding emotion expressed in language has a wide range of applications, from building empathetic chatbots to detecting harmful online behavior. Advancement in this area can be improved using large-scale datasets with a fine-grained typology, adaptable to multiple downstream tasks. We introduce {GoEmotions}, the largest manually annotated dataset of 58k English Reddit comments, labeled for 27 emotion categories or Neutral. We demonstrate the high quality of the annotations via Principal Preserved Component Analysis. We conduct transfer learning experiments with existing emotion benchmarks to show that our dataset generalizes well to other domains and different emotion taxonomies. Our {BERT}-based model achieves an average F1-score of .46 across our proposed taxonomy, leaving much room for improvement.},
	number = {{arXiv}:2005.00547},
	publisher = {{arXiv}},
	author = {Demszky, Dorottya and Movshovitz-Attias, Dana and Ko, Jeongwoo and Cowen, Alan and Nemade, Gaurav and Ravi, Sujith},
	urldate = {2023-02-27},
	date = {2020-06-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.00547 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@online{noauthor_embed_2016,
	title = {Embed, encode, attend, predict: The new deep learning formula for state-of-the-art {NLP} models · Explosion},
	url = {https://explosion.ai/blog/deep-learning-formula-nlp},
	shorttitle = {Embed, encode, attend, predict},
	abstract = {Over the last six months, a powerful new neural network playbook has come together for Natural Language Processing. The new approach can be summarised as a simple four-step formula: embed, encode, attend, predict. This post explains the components of this new approach, and shows how they're put together in two recent systems.},
	titleaddon = {Explosion},
	urldate = {2023-02-27},
	date = {2016-11-10},
	langid = {english},
}

@misc{joulin_fasttextzip_2016,
	title = {{FastText}.zip: Compressing text classification models},
	url = {http://arxiv.org/abs/1612.03651},
	doi = {10.48550/arXiv.1612.03651},
	shorttitle = {{FastText}.zip},
	abstract = {We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory. After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store word embeddings. While the original technique leads to a loss in accuracy, we adapt this method to circumvent quantization artefacts. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than {fastText} while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.},
	number = {{arXiv}:1612.03651},
	publisher = {{arXiv}},
	author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and Jégou, Hérve and Mikolov, Tomas},
	urldate = {2023-02-26},
	date = {2016-12-12},
	eprinttype = {arxiv},
	eprint = {1612.03651 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{joulin_bag_2016,
	title = {Bag of Tricks for Efficient Text Classification},
	url = {http://arxiv.org/abs/1607.01759},
	doi = {10.48550/arXiv.1607.01759},
	abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier {fastText} is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train {fastText} on more than one billion words in less than ten minutes using a standard multicore{\textasciitilde}{CPU}, and classify half a million sentences among{\textasciitilde}312K classes in less than a minute.},
	number = {{arXiv}:1607.01759},
	publisher = {{arXiv}},
	author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
	urldate = {2023-02-26},
	date = {2016-08-09},
	eprinttype = {arxiv},
	eprint = {1607.01759 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{bojanowski_enriching_2017,
	title = {Enriching Word Vectors with Subword Information},
	url = {http://arxiv.org/abs/1607.04606},
	abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
	number = {{arXiv}:1607.04606},
	publisher = {{arXiv}},
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	urldate = {2023-02-26},
	date = {2017-06-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1607.04606 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{tymann_gervader_nodate,
	title = {{GerVADER} - A German adaptation of the {VADER} sentiment analysis tool for social media texts},
	abstract = {For the English language sentiment analysis tools are fairly popular. One is called {VADER} [1] which oﬀers a rather simple process for sentiment classiﬁcation. Due to its lexicon-based approach with a design focus on social media texts, no additional training data is required. In this paper the process of creating {VADER} is applied to build a German adaptation which is called {GerVADER}. The paper will present the concept of {VADER} and how a German version can be built within reasonable time. {GerVADER} uses {SentiWS} as a starting point for the lexicon, combines it with language independent parts of the {VADER} lexicon and copies the process of having users rate the words intensity and polarity. The next step is comprised of comparing the algorithmically changes due to the natural diﬀerences in language between German and English. Then {GerVADER} is compared to the results of the {SB}10k [2] corpus classiﬁcation which contains more than 9000 human labeled tweets. Finally {GerVADER} is tested with parts of the {SCARE} [3] dataset which contains reviews for mobile apps. The results show that {GerVADER} lacks some additional work to increase its classiﬁcation accuracy, but it promises better results considering how well the original performed.},
	author = {Tymann, Karsten Michael and Lutz, Matthias and Palsbroker, Patrick and Gips, Carsten},
	langid = {english},
}

@inproceedings{remus_sentiws_2010,
	location = {Valletta, Malta},
	title = {{SentiWS} - A Publicly Available German-language Resource for Sentiment Analysis},
	url = {http://www.lrec-conf.org/proceedings/lrec2010/pdf/490_Paper.pdf},
	abstract = {{SentimentWortschatz}, or {SentiWS} for short, is a publicly available German-language resource for sentiment analysis, opinion mining etc. It lists positive and negative sentiment bearing words weighted within the interval of [-1; 1] plus their part of speech tag, and if applicable, their inflections. The current version of {SentiWS} (v1.8b) contains 1,650 negative and 1,818 positive words, which sum up to 16,406 positive and 16,328 negative word forms, respectively. It not only contains adjectives and adverbs explicitly expressing a sentiment, but also nouns and verbs implicitly containing one. The present work describes the resources structure, the three sources utilised to assemble it and the semi-supervised method incorporated to weight the strength of its entries. Furthermore the resources contents are extensively evaluated using a German-language evaluation set we constructed. The evaluation set is verified being reliable and its shown that {SentiWS} provides a beneficial lexical resource for German-language sentiment analysis related tasks to build on.},
	eventtitle = {{LREC} 2010},
	booktitle = {Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)},
	publisher = {European Language Resources Association ({ELRA})},
	author = {Remus, Robert and Quasthoff, Uwe and Heyer, Gerhard},
	urldate = {2023-02-22},
	date = {2010-05},
}

@inproceedings{chan_germans_2020,
	location = {Barcelona, Spain (Online)},
	title = {German’s Next Language Model},
	url = {https://www.aclweb.org/anthology/2020.coling-main.598},
	doi = {10.18653/v1/2020.coling-main.598},
	abstract = {In this work we present the experiments which lead to the creation of our {BERT} and {ELECTRA} based German language models, {GBERT} and {GELECTRA}. By varying the input training data, model size, and the presence of Whole Word Masking ({WWM}) we were able to attain {SoTA} performance across a set of document classiﬁcation and named entity recognition ({NER}) tasks for both models of base and large size. We adopt an evaluation driven approach in training these models and our results indicate that both adding more data and utilizing {WWM} improve model performance. By benchmarking against existing German models, we show that these models are the best German models to date. Our trained models will be made publicly available to the research community.},
	eventtitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	pages = {6788--6796},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	publisher = {International Committee on Computational Linguistics},
	author = {Chan, Branden and Schweter, Stefan and Möller, Timo},
	urldate = {2023-02-21},
	date = {2020},
	langid = {english},
}

@misc{martinez-plumed_casp-dm_2017,
	title = {{CASP}-{DM}: Context Aware Standard Process for Data Mining},
	url = {http://arxiv.org/abs/1709.09003},
	shorttitle = {{CASP}-{DM}},
	abstract = {We propose an extension of the Cross Industry Standard Process for Data Mining ({CRISPDM}) which addresses specific challenges of machine learning and data mining for context and model reuse handling. This new general context-aware process model is mapped with {CRISP}-{DM} reference model proposing some new or enhanced outputs.},
	number = {{arXiv}:1709.09003},
	publisher = {{arXiv}},
	author = {Martínez-Plumed, Fernando and Contreras-Ochando, Lidia and Ferri, Cèsar and Flach, Peter and Hernández-Orallo, José and Kull, Meelis and Lachiche, Nicolas and Ramírez-Quintana, María José},
	urldate = {2023-02-21},
	date = {2017-09-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1709.09003 [cs]},
	keywords = {Computer Science - Databases},
}

@article{ribeiro_media_2018,
	title = {Media Bias Monitor: Quantifying Biases of Social Media News Outlets at Large-Scale},
	volume = {12},
	rights = {Copyright (c) 2022 Proceedings of the International {AAAI} Conference on Web and Social Media},
	issn = {2334-0770},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/15025},
	doi = {10.1609/icwsm.v12i1.15025},
	shorttitle = {Media Bias Monitor},
	abstract = {As Internet users increasingly rely on social media sites like Facebook and Twitter to receive news, they are faced with a bewildering number of news media choices. For example, thousands of Facebook pages today are registered and categorized as some form of news media outlets. Inferring the bias (or slant) of these media pages poses a difficult challenge for media watchdog organizations that traditionally rely on content analysis. In this paper, we explore a novel scalable methodology to accurately infer the biases of thousands of news sources on social media sites like Facebook and Twitter. Our key idea is to utilize their advertiser interfaces, that offer detailed insights into the demographics of the news source’s audience on the social media site. We show that the ideological (liberal or conservative) leaning of a news source can be accurately estimated by the extent to which liberals or conservatives are over-/under-represented among its audience. Additionally, we show how biases in a news source’s audience demographics, along the lines of race, gender, age, national identity, and income, can be used to infer more fine-grained biases of the source, such as social vs. economic vs. nationalistic conservatism. Finally, we demonstrate the scalability of our approach by building and publicly deploying a system, called "Media Bias Monitor", which makes the biases in audience demographics for over 20,000 news outlets on Facebook transparent to any Internet user.},
	number = {1},
	journaltitle = {Proceedings of the International {AAAI} Conference on Web and Social Media},
	author = {Ribeiro, Filipe and Henrique, Lucas and Benevenuto, Fabricio and Chakraborty, Abhijnan and Kulshrestha, Juhi and Babaei, Mahmoudreza and Gummadi, Krishna},
	urldate = {2023-02-20},
	date = {2018-06-15},
	langid = {english},
	note = {Number: 1},
	keywords = {Facebook audience demographics},
}

@online{noauthor_lieblingsmedien_nodate,
	title = {Die Lieblingsmedien der Parteien im Bundestag},
	url = {https://interaktiv.tagesspiegel.de/lab/die-lieblingsmedien-der-parteien/},
	abstract = {Die Mehrheit der Menschen bezieht ihre Nachrichten online. Welche News verbreitet die Politik auf Social Media? Eine Analyse von über 400.000 Posts auf Twitter und Facebook.},
	titleaddon = {Tagesspiegel},
	urldate = {2023-02-20},
	langid = {german},
}

@online{writer_study_2020,
	title = {Study finds political bias skews perceptions of verifiable fact},
	url = {https://news.harvard.edu/gazette/story/2020/06/study-finds-political-bias-skews-perceptions-of-verifiable-fact/},
	abstract = {New research from Harvard economists finds partisan politics isn’t just shaping policy opinions, it’s distorting our understanding of reality.},
	titleaddon = {Harvard Gazette},
	author = {Writer, Christina Pazzanese Harvard Staff},
	urldate = {2023-02-20},
	date = {2020-06-03},
	langid = {american},
	note = {Section: National \& World Affairs},
}

@misc{baly_we_2020,
	title = {We Can Detect Your Bias: Predicting the Political Ideology of News Articles},
	url = {http://arxiv.org/abs/2010.05338},
	doi = {10.48550/arXiv.2010.05338},
	shorttitle = {We Can Detect Your Bias},
	abstract = {We explore the task of predicting the leading political ideology or bias of news articles. First, we collect and release a large dataset of 34,737 articles that were manually annotated for political ideology -left, center, or right-, which is well-balanced across both topics and media. We further use a challenging experimental setup where the test examples come from media that were not seen during training, which prevents the model from learning to detect the source of the target news article instead of predicting its political ideology. From a modeling perspective, we propose an adversarial media adaptation, as well as a specially adapted triplet loss. We further add background information about the source, and we show that it is quite helpful for improving article-level prediction. Our experimental results show very sizable improvements over using state-of-the-art pre-trained Transformers in this challenging setup.},
	number = {{arXiv}:2010.05338},
	publisher = {{arXiv}},
	author = {Baly, Ramy and Martino, Giovanni Da San and Glass, James and Nakov, Preslav},
	urldate = {2023-02-20},
	date = {2020-10-11},
	eprinttype = {arxiv},
	eprint = {2010.05338 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{malouf_taking_2008,
	title = {Taking sides: user classification for informal online political discourse},
	volume = {18},
	issn = {1066-2243},
	url = {https://doi.org/10.1108/10662240810862239},
	doi = {10.1108/10662240810862239},
	shorttitle = {Taking sides},
	abstract = {Purpose – To evaluate and extend, existing natural language processing techniques into the domain of informal online political discussions. Design/methodology/approach – A database of postings from a {US} political discussion site was collected, along with self‐reported political orientation data for the users. A variety of sentiment analysis, text classification, and social network analysis methods were applied to the postings and evaluated against the users' self‐descriptions. Findings – Purely text‐based methods performed poorly, but could be improved using techniques which took into account the users' position in the online community. Research limitations/implications – The techniques we applied here are fairly simple, and more sophisticated learning algorithms may yield better results for text‐based classification. Practical implications – This work suggests that social network analysis is an important tool for performing natural language processing tasks with informal web texts. Originality/value – This research extends sentiment analysis to a new subject domain ({US} politics) and a new text genre (informal online discusssions).},
	pages = {177--190},
	number = {2},
	journaltitle = {Internet Research},
	author = {Malouf, Robert and Mullen, Tony},
	editor = {Kato, Yoshikiyo and Kurohashi, Sadao and Inui, Kentaro},
	urldate = {2023-02-20},
	date = {2008-01-01},
	note = {Publisher: Emerald Group Publishing Limited},
	keywords = {Databases, Online operations, Politics, United States of America},
}

@article{bhagat_indeprop_2022,
	title = {{INDEPROP}: Information-Preserving De-propagandization of News Articles (Student Abstract)},
	volume = {36},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/21594},
	doi = {10.1609/aaai.v36i11.21594},
	shorttitle = {{INDEPROP}},
	abstract = {We propose {INDEPROP}, a novel Natural Language Processing ({NLP}) application for combating online disinformation by mitigating propaganda from news articles. {INDEPROP} (Information-Preserving De-propagandization) involves fine-grained propaganda detection and its removal while maintaining document level coherence, grammatical correctness and most importantly, preserving the news articles’ information content. We curate the first large-scale dataset of its kind consisting of around 1M tokens. We also propose a set of automatic evaluation metrics for the same and observe its high correlation with human judgment. Furthermore, we show that fine-tuning the existing propaganda detection systems on our dataset considerably improves their generalization to the test set.},
	pages = {12915--12916},
	number = {11},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Bhagat, Aaryan and Mallick, Faraaz and Karia, Neel and Kaushal, Ayush},
	urldate = {2023-02-20},
	date = {2022-06-28},
}

@online{barbaresi_german_nodate,
	title = {German Political Speeches Corpus and Visualization},
	url = {https://politische-reden.eu/},
	author = {Barbaresi, Adrien},
	urldate = {2023-02-20},
}

@inproceedings{guhr_training_2020,
	location = {Marseille, France},
	title = {Training a Broad-Coverage German Sentiment Classification Model for Dialog Systems},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.202},
	abstract = {This paper describes the training of a general-purpose German sentiment classification model. Sentiment classification is an important aspect of general text analytics. Furthermore, it plays a vital role in dialogue systems and voice interfaces that depend on the ability of the system to pick up and understand emotional signals from user utterances. The presented study outlines how we have collected a new German sentiment corpus and then combined this corpus with existing resources to train a broad-coverage German sentiment model. The resulting data set contains 5.4 million labelled samples. We have used the data to train both, a simple convolutional and a transformer-based classification model and compared the results achieved on various training configurations. The model and the data set will be published along with this paper.},
	eventtitle = {{LREC} 2020},
	pages = {1627--1632},
	booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
	publisher = {European Language Resources Association},
	author = {Guhr, Oliver and Schumann, Anne-Kathrin and Bahrmann, Frank and Böhme, Hans Joachim},
	urldate = {2023-02-20},
	date = {2020-05},
}

@article{gimpel_user_2018,
	title = {User roles in online political discussions: A typology based on Twitter data from the German Federal Election 2017},
	url = {https://www.fim-rc.de/Paperbibliothek/Veroeffentlicht/743/wi-743.pdf},
	shorttitle = {User roles in online political discussions},
	abstract = {Twitter is well recognized as a microblogging site, an online social network ({OSN}), and increasingly as a digital news platform. With the changing media usage behavior over the past decade, political actors have now recognized the need to enrich their election campaign efforts by including social media strategies. However, previous research has shown that users behave heterogeneously in online political discussions. To better understand how users behave and interact in such debates, we conduct an exploratory study to identify emergent user roles from Twitter data. We develop a dynamic selection query to collect a representative data set on the German federal election of 2017. We define features of structure, function, and time for Twitter discussions and conduct a cluster analysis to derive eleven emergent roles from the 30,553 most active users. We then refine those roles by further data-driven analyses to enhance and deepen their understanding. Our results indicate dominance of the online discussion by the populist party Alternative fu\&r Deutschland. We also find that media outlets and political parties show somewhat similar behavior, and that the offline popularity of prestigious actors is extended into the online world.},
	author = {Gimpel, Henner and Haamann, Florian and Schoch, Manfred and Wittich, Marcel},
	urldate = {2023-02-18},
	date = {2018},
	langid = {english},
}

@book{zhang_dive_nodate,
	title = {Dive into Deep Learning},
	url = {https://d2l.ai/d2l-en.pdf},
	author = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
}

@inproceedings{gangula_detecting_2019,
	location = {Florence, Italy},
	title = {Detecting Political Bias in News Articles Using Headline Attention},
	url = {https://www.aclweb.org/anthology/W19-4809},
	doi = {10.18653/v1/W19-4809},
	abstract = {Language is a powerful tool which can be used to state the facts as well as express our views and perceptions. Most of the times, we ﬁnd a subtle bias towards or against someone or something. When it comes to politics, media houses and journalists are known to create bias by shrewd means such as misinterpreting reality and distorting viewpoints towards some parties. This misinterpretation on a large scale can lead to the production of biased news and conspiracy theories. Automating bias detection in newspaper articles could be a good challenge for research in {NLP}.},
	eventtitle = {Proceedings of the 2019 {ACL} Workshop {BlackboxNLP}: Analyzing and Interpreting Neural Networks for {NLP}},
	pages = {77--84},
	booktitle = {Proceedings of the 2019 {ACL} Workshop {BlackboxNLP}: Analyzing and Interpreting Neural Networks for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Gangula, Rama Rohit Reddy and Duggenpudi, Suma Reddy and Mamidi, Radhika},
	urldate = {2023-02-16},
	date = {2019},
	langid = {english},
}

@online{pietro_text_2022,
	title = {Text Classification with {NLP}: Tf-Idf vs Word2Vec vs {BERT}},
	url = {https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794},
	shorttitle = {Text Classification with {NLP}},
	abstract = {Preprocessing, Model Design, Evaluation, Explainability for Bag-of-Words, Word Embedding, Language models},
	titleaddon = {Medium},
	author = {Pietro, Mauro Di},
	urldate = {2023-02-16},
	date = {2022-03-22},
	langid = {english},
}

@inproceedings{sundermeyer_lstm_2012,
	title = {{LSTM} neural networks for language modeling},
	url = {https://www.isca-speech.org/archive/interspeech_2012/sundermeyer12_interspeech.html},
	doi = {10.21437/Interspeech.2012-65},
	eventtitle = {Interspeech 2012},
	pages = {194--197},
	booktitle = {Interspeech 2012},
	publisher = {{ISCA}},
	author = {Sundermeyer, Martin and Schlüter, Ralf and Ney, Hermann},
	urldate = {2023-02-16},
	date = {2012-09-09},
	langid = {english},
}

@article{minaee_deep_2022,
	title = {Deep Learning-based Text Classification: A Comprehensive Review},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3439726},
	doi = {10.1145/3439726},
	shorttitle = {Deep Learning--based Text Classification},
	abstract = {Deep learning--based models have surpassed classical machine learning--based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this article, we provide a comprehensive review of more than 150 deep learning--based models for text classification developed in recent years, and we discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and we discuss future research directions.},
	pages = {1--40},
	number = {3},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Minaee, Shervin and Kalchbrenner, Nal and Cambria, Erik and Nikzad, Narjes and Chenaghlu, Meysam and Gao, Jianfeng},
	urldate = {2023-01-03},
	date = {2022-04-30},
	langid = {english},
}

@incollection{carbonell_text_1998,
	location = {Berlin, Heidelberg},
	title = {Text categorization with Support Vector Machines: Learning with many relevant features},
	volume = {1398},
	isbn = {978-3-540-64417-0 978-3-540-69781-7},
	url = {http://link.springer.com/10.1007/BFb0026683},
	shorttitle = {Text categorization with Support Vector Machines},
	pages = {137--142},
	booktitle = {Machine Learning: {ECML}-98},
	publisher = {Springer Berlin Heidelberg},
	author = {Joachims, Thorsten},
	editor = {Nédellec, Claire and Rouveirol, Céline},
	editorb = {Carbonell, Jaime G. and Siekmann, Jörg and Goos, G. and Hartmanis, J. and van Leeuwen, J.},
	editorbtype = {redactor},
	urldate = {2023-02-16},
	date = {1998},
	doi = {10.1007/BFb0026683},
	note = {Series Title: Lecture Notes in Computer Science},
}

@article{jha_towards_2023,
	title = {Towards automated check-worthy sentence detection using Gated Recurrent Unit},
	issn = {0941-0643, 1433-3058},
	url = {https://link.springer.com/10.1007/s00521-023-08300-x},
	doi = {10.1007/s00521-023-08300-x},
	journaltitle = {Neural Computing and Applications},
	shortjournal = {Neural Comput \& Applic},
	author = {Jha, Ria and Motwani, Ena and Singhal, Nivedita and Kaushal, Rishabh},
	urldate = {2023-02-16},
	date = {2023-02-10},
	langid = {english},
}

@inproceedings{tusar_comparative_2021,
	title = {A Comparative Study of Sentiment Analysis Using {NLP} and Different Machine Learning Techniques on {US} Airline Twitter Data},
	doi = {10.1109/ICECIT54077.2021.9641336},
	abstract = {Today's business ecosystem has become very competitive. Customer satisfaction has become a major focus for business growth. Business organizations are spending a lot of money and human resources on various strategies to understand and fulfill their customer's needs. But, because of defective manual analysis on multifarious needs of customers, many organizations are failing to achieve customer satisfaction. As a result, they are losing customer's loyalty and spending extra money on marketing. We can solve the problems by implementing Sentiment Analysis. It is a combined technique of Natural Language Processing ({NLP}) and Machine Learning ({ML}). Sentiment Analysis is broadly used to extract insights from wider public opinion behind certain topics, products, and services. We can do it from any online available data. In this paper, we have introduced two {NLP} techniques (Bag-of-Words and {TF}-{IDF}) and various {ML} classification algorithms (Support Vector Machine, Logistic Regression, Multinomial Naive Bayes, Random Forest) to find an effective approach for Sentiment Analysis on a large, imbalanced, and multi-classed dataset. Our best approaches provide 77\% accuracy using Support Vector Machine and Logistic Regression with Bag-of-Words technique.},
	eventtitle = {2021 International Conference on Electronics, Communications and Information Technology ({ICECIT})},
	pages = {1--4},
	author = {Tusar, Md Taufiqul Haque Khan and Islam, Md. Touhidul},
	date = {2021-09},
	keywords = {Airline, Classification algorithms, Customer satisfaction, Logistic Regression, Machine Learning, Manuals, Organizations, {SVM}, Sentiment Analysis, Sentiment analysis, Social networking (online), Support vector machines, Twitter},
}

@article{saltzer_bundestagswahl_2022,
	title = {Die Bundestagswahl 2021 auf Twitter},
	volume = {No. 67},
	rights = {Creative Commons Attribution 4.0 International},
	issn = {2749-2850},
	url = {https://www.ssoar.info/ssoar/handle/document/79902},
	doi = {10.15464/EASY.2022.05},
	journaltitle = {easy social sciences},
	author = {Sältzer, Marius and Stier, Sebastian},
	editora = {{GESIS}-Leibniz-Institut Für Sozialwissenschaften},
	editoratype = {collaborator},
	urldate = {2023-01-07},
	date = {2022},
	langid = {german},
	note = {Publisher: {GESIS} - Leibniz-Institut für Sozialwissenschaften
Version Number: 1},
	keywords = {2021, Bundestagswahl, Twitter},
}

@online{lee_how_2020,
	title = {How Fake News Affects U.S. Elections},
	url = {https://www.ucf.edu/news/how-fake-news-affects-u-s-elections/},
	titleaddon = {University of Central Florida},
	author = {Lee, Jenna Marina},
	urldate = {2023-01-07},
	date = {2020-10-26},
	langid = {english},
	note = {Section: Colleges \& Campus},
}

@online{brandon_russia_2022,
	title = {Russia Has Radically Redefined The Term ‘Fake News’ During The Ukraine War},
	url = {https://www.forbes.com/sites/johnbbrandon/2022/07/31/russia-has-radically-redefined-the-term-fake-news-during-the-ukraine-war/},
	titleaddon = {Forbes},
	author = {Brandon, John},
	urldate = {2023-01-07},
	date = {2022-07-31},
	langid = {english},
	note = {Section: Social Media},
}

@inproceedings{doan_using_2022,
	location = {Cham},
	title = {Using Language Models for Classifying the Party Affiliation of Political Texts},
	isbn = {978-3-031-08473-7},
	doi = {10.1007/978-3-031-08473-7_35},
	series = {Lecture Notes in Computer Science},
	abstract = {We analyze the use of language models for political text classification. Political texts become increasingly available and language models have succeeded in various natural language processing tasks. We apply two baselines and different language models to data from the {UK}, Germany, and Norway. Observed accuracy shows language models improving on the performance of the baselines by up to 10.35\% (Norwegian), 12.95\% (German), and 6.39\% (English).},
	pages = {382--393},
	booktitle = {Natural Language Processing and Information Systems},
	publisher = {Springer International Publishing},
	author = {Doan, Tu My and Kille, Benjamin and Gulla, Jon Atle},
	editor = {Rosso, Paolo and Basile, Valerio and Martínez, Raquel and Métais, Elisabeth and Meziane, Farid},
	date = {2022},
	langid = {english},
	keywords = {Language models, Party affiliation classification, Political text representation},
}

@article{wong_quantifying_2016,
	title = {Quantifying Political Leaning from Tweets, Retweets, and Retweeters},
	volume = {28},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2016.2553667},
	abstract = {The widespread use of online social networks ({OSNs}) to disseminate information and exchange opinions, by the general public, news media, and political actors alike, has enabled new avenues of research in computational political science. In this paper, we study the problem of quantifying and inferring the political leaning of Twitter users. We formulate political leaning inference as a convex optimization problem that incorporates two ideas: (a) users are consistent in their actions of tweeting and retweeting about political issues, and (b) similar users tend to be retweeted by similar audience. We then apply our inference technique to 119 million election-related tweets collected in seven months during the 2012 U.S. presidential election campaign. On a set of frequently retweeted sources, our technique achieves 94 percent accuracy and high rank correlation as compared with manually created labels. By studying the political leaning of 1,000 frequently retweeted sources, 232,000 ordinary users who retweeted them, and the hashtags used by these sources, our quantitative study sheds light on the political demographics of the Twitter population, and the temporal dynamics of political polarization as events unfold.},
	pages = {2158--2172},
	number = {8},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Wong, Felix Ming Fai and Tan, Chee Wei and Sen, Soumya and Chiang, Mung},
	date = {2016-08},
	note = {Conference Name: {IEEE} Transactions on Knowledge and Data Engineering},
	keywords = {Electronic mail, Media, Nominations and elections, Sociology, Statistics, Twitter, convex programming, data analytics, inference, political science, signal processing},
}

@article{kowsari_text_2019,
	title = {Text Classification Algorithms: A Survey},
	volume = {10},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2078-2489},
	url = {https://www.mdpi.com/2078-2489/10/4/150},
	doi = {10.3390/info10040150},
	shorttitle = {Text Classification Algorithms},
	abstract = {In recent years, there has been an exponential growth in the number of complex documents and texts that require a deeper understanding of machine learning methods to be able to accurately classify texts in many applications. Many machine learning approaches have achieved surpassing results in natural language processing. The success of these learning algorithms relies on their capacity to understand complex models and non-linear relationships within data. However, finding suitable structures, architectures, and techniques for text classification is a challenge for researchers. In this paper, a brief overview of text classification algorithms is discussed. This overview covers different text feature extractions, dimensionality reduction methods, existing algorithms and techniques, and evaluations methods. Finally, the limitations of each technique and their application in real-world problems are discussed.},
	pages = {150},
	number = {4},
	journaltitle = {Information},
	author = {Kowsari, Kamran and Jafari Meimandi, Kiana and Heidarysafa, Mojtaba and Mendu, Sanjana and Barnes, Laura and Brown, Donald},
	urldate = {2023-01-03},
	date = {2019-04},
	langid = {english},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {document classification, text analysis, text categorization, text classification, text mining, text representation},
}

@misc{li_survey_2021,
	title = {A Survey on Text Classification: From Shallow to Deep Learning},
	url = {http://arxiv.org/abs/2008.00364},
	shorttitle = {A Survey on Text Classification},
	abstract = {Text classification is the most fundamental and essential task in natural language processing. The last decade has seen a surge of research in this area due to the unprecedented success of deep learning. Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey. This paper fills the gap by reviewing the state-of-the-art approaches from 1961 to 2021, focusing on models from traditional models to deep learning. We create a taxonomy for text classification according to the text involved and the models used for feature extraction and classification. We then discuss each of these categories in detail, dealing with both the technical developments and benchmark datasets that support tests of predictions. A comprehensive comparison between different techniques, as well as identifying the pros and cons of various evaluation metrics are also provided in this survey. Finally, we conclude by summarizing key implications, future research directions, and the challenges facing the research area.},
	number = {{arXiv}:2008.00364},
	publisher = {{arXiv}},
	author = {Li, Qian and Peng, Hao and Li, Jianxin and Xia, Congying and Yang, Renyu and Sun, Lichao and Yu, Philip S. and He, Lifang},
	urldate = {2023-01-03},
	date = {2021-12-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2008.00364 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{kalyanam_prediction_2016,
	title = {Prediction and Characterization of High-Activity Events in Social Media Triggered by Real-World News},
	volume = {11},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0166694},
	doi = {10.1371/journal.pone.0166694},
	abstract = {On-line social networks publish information on a high volume of real-world events almost instantly, becoming a primary source for breaking news. Some of these real-world events can end up having a very strong impact on on-line social networks. The effect of such events can be analyzed from several perspectives, one of them being the intensity and characteristics of the collective activity that it produces in the social platform. We research 5,234 real-world news events encompassing 43 million messages discussed on the Twitter microblogging service for approximately 1 year. We show empirically that exogenous news events naturally create collective patterns of bursty behavior in combination with long periods of inactivity in the network. This type of behavior agrees with other patterns previously observed in other types of natural collective phenomena, as well as in individual human communications. In addition, we propose a methodology to classify news events according to the different levels of intensity in activity that they produce. In particular, we analyze the most highly active events and observe a consistent and strikingly different collective reaction from users when they are exposed to such events. This reaction is independent of an event’s reach and scope. We further observe that extremely high-activity events have characteristics that are quite distinguishable at the beginning stages of their outbreak. This allows us to predict with high precision, the top 8\% of events that will have the most impact in the social network by just using the first 5\% of the information of an event’s lifetime evolution. This strongly implies that high-activity events are naturally prioritized collectively by the social network, engaging users early on, way before they are brought to the mainstream audience.},
	number = {12},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Kalyanam, Janani and Quezada, Mauricio and Poblete, Barbara and Lanckriet, Gert},
	urldate = {2022-09-10},
	date = {2016-12-16},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Collective human behavior, Communications, Mass media, Microbial evolution, Social media, Social networks, Tornadoes, Twitter, notion},
}

@article{saltzer_finding_2022,
	title = {Finding the bird’s wings: Dimensions of factional conflict on Twitter},
	volume = {28},
	issn = {1354-0688, 1460-3683},
	url = {http://journals.sagepub.com/doi/10.1177/1354068820957960},
	doi = {10.1177/1354068820957960},
	shorttitle = {Finding the bird’s wings},
	abstract = {Intra-party politics has long been neglected due to lacking data sources. While we have a good understanding of the dynamics of ideological competition between parties, we know less about how individuals or groups inside parties influence policy, leadership selection and coalition bargaining. These questions can only be answered if we can place individual politicians and sub-party groups like factions on the same dimensions as in inter-party competition. This task has been notoriously difficult, as most existing measures either work on the party level, or are in other ways determined by the party agenda. Social media is a new data source that allows analyzing positions of individual politicians in party-centered systems, as it is subject to limited party control. I apply canonical correspondence analysis to account for hierarchical data structures and estimate multidimensional positions of the Twitter accounts of 498 Members of the German Bundestag based on more than 800,000 tweets since 2017. To test the effect of intra-party actors on their relative ideological placement, I coded the faction membership of 247 Twitter users in the Bundestag. I show that Twitter text reproduces party positions and dimensions. Members of factions are more likely to represent their faction’s positions, both on the cultural and the economic dimension.},
	pages = {61--70},
	number = {1},
	journaltitle = {Party Politics},
	shortjournal = {Party Politics},
	author = {Sältzer, Marius},
	urldate = {2022-10-22},
	date = {2022-01},
	langid = {english},
}

@inproceedings{tsytsarau_dynamics_2014,
	location = {New York, {NY}, {USA}},
	title = {Dynamics of news events and social media reaction},
	isbn = {978-1-4503-2956-9},
	url = {https://doi.org/10.1145/2623330.2623670},
	doi = {10.1145/2623330.2623670},
	series = {{KDD} '14},
	abstract = {The analysis of social sentiment expressed on the Web is becoming increasingly relevant to a variety of applications, and it is important to understand the underlying mechanisms which drive the evolution of sentiments in one way or another, in order to be able to predict these changes in the future. In this paper, we study the dynamics of news events and their relation to changes of sentiment expressed on relevant topics. We propose a novel framework, which models the behavior of news and social media in response to events as a convolution between event's importance and media response function, specific to media and event type. This framework is suitable for detecting time and duration of events, as well as their impact and dynamics, from time series of publication volume. These data can greatly enhance events analysis; for instance, they can help distinguish important events from unimportant, or predict sentiment and stock market shifts. As an example of such application, we extracted news events for a variety of topics and then correlated this data with the corresponding sentiment time series, revealing the connection between sentiment shifts and event dynamics.},
	pages = {901--910},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Tsytsarau, Mikalai and Palpanas, Themis and Castellanos, Malu},
	urldate = {2022-09-10},
	date = {2014-08-24},
	keywords = {information spread, news dynamics, notion, sentiment analysis, social media},
}
